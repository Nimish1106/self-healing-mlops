version: '3.8'

services:
  # ============================================================
  # MLflow Tracking Server
  # ============================================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    container_name: mlflow-server
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow:/mlflow
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ============================================================
  # Model Training (one-time execution)
  # ============================================================
  trainer:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: model-trainer
    volumes:
      - ./data:/app/data
      - ./mlflow:/mlflow
      - ./models:/app/models
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    # depends_on:
    #   mlflow:
    #     condition: service_healthy
    networks:
      - mlops-network
    command: python src/train_model_mlflow.py
    restart: "no"  # Run once and exit

  # ============================================================
  # Reference Data Bootstrap (one-time execution)
  # ============================================================
  bootstrap:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: reference-bootstrap
    volumes:
      - ./data:/app/data
      - ./monitoring:/app/monitoring
    networks:
      - mlops-network
    command: python scripts/bootstrap_reference.py
    restart: "no"  # Run once and exit

  # ============================================================
  # Prediction API (long-running service)
  # ============================================================
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: credit-risk-api
    ports:
      - "8000:8000"
    volumes:
      - ./mlflow:/mlflow
      - ./monitoring:/app/monitoring
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - ENVIRONMENT=production
    # depends_on:
    #   mlflow:
    #     condition: service_healthy
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    command: uvicorn src.api_mlflow:app --host 0.0.0.0 --port 8000

  # ============================================================
  # Monitoring Scheduler (long-running service)
  # Phase 3 addition: runs monitoring jobs periodically
  # ============================================================
  monitoring:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: monitoring-scheduler
    volumes:
      - ./monitoring:/app/monitoring
      - ./mlflow:/mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MONITORING_INTERVAL=300      # Run every 5 minutes (300 seconds)
      - MONITORING_LOOKBACK=24       # Analyze last 24 hours
    # depends_on:
    #   mlflow:
    #     condition: service_healthy
    #   api:
    #     condition: service_healthy
    networks:
      - mlops-network
    restart: unless-stopped
    command: python src/orchestration/scheduler.py

networks:
  mlops-network:
    driver: bridge

# ============================================================
# Volumes explanation:
# - mlflow/: MLflow tracking data (persisted)
# - monitoring/: All monitoring data (predictions, reference, reports)
# - data/: Training datasets (mounted read-only)
# - models/: Local model storage (fallback only)
# ============================================================
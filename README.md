# Self-Healing MLOps: Credit Risk Model with Continuous Monitoring & Retraining

A **production-grade ML system** that autonomously detects model degradation, evaluates retraining candidates, and makes promotion decisionsâ€”all while maintaining data integrity and fairness constraints.

## ðŸŽ¯ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SELF-HEALING MLOPS PIPELINE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Phase 1: Training         â”‚  Phase 2: Production Serving        â”‚
â”‚  â€¢ MLflow tracking         â”‚  â€¢ FastAPI credit risk API          â”‚
â”‚  â€¢ Model versioning        â”‚  â€¢ Prediction logging               â”‚
â”‚  â€¢ Hyperparameter tuning   â”‚  â€¢ Label collection                 â”‚
â”‚                            â”‚                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                            â”‚                                      â”‚
â”‚  Phase 3: Monitoring       â”‚  Phase 4: Decision & Retraining     â”‚
â”‚  â€¢ Drift detection         â”‚  â€¢ Evaluation gates                  â”‚
â”‚  â€¢ Proxy metrics           â”‚  â€¢ Shadow model training            â”‚
â”‚  â€¢ Evidently reports       â”‚  â€¢ Model promotion logic            â”‚
â”‚                            â”‚  â€¢ Cooldown enforcement             â”‚
â”‚                            â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ—ï¸ System Components

### **Phase 1: Model Training** (`src/train_model_mlflow.py`)
- Baseline model development using scikit-learn
- Hyperparameter tuning with GridSearchCV
- MLflow experiment tracking (metrics, params, artifacts)
- Model versioning with production stage tags

### **Phase 2: Production API** (`src/api_mlflow.py`)
- FastAPI service serving production model
- Real-time credit risk predictions (0-1 probability)
- Prediction logging with timestamps and features
- Inference telemetry for downstream monitoring

### **Phase 3: Drift Monitoring** (`src/analytics/drift_detection.py`)
**Evidently v0.4.15 statistical drift detection**
- **DatasetDriftMetric**: Overall distribution change detection
- **ColumnDriftMetric**: Feature-level drift using Wasserstein distance
- Reference baseline (frozen 30K samples from training set)
- 24-hour lookback window for current data
- HTML drift reports + JSON summaries with complete feature details

**Drift Summary Structure** (example):
```json
{
  "timestamp": "2026-01-04T09:33:59.180318",
  "dataset_drift_detected": true,
  "drift_share": 0.5,
  "num_drifted_features": 6,
  "num_features_total": 10,
  "num_features_evaluated": 10,
  "excluded_features": [],
  "features": [
    {
      "feature": "MonthlyIncome",
      "drift_detected": true,
      "stat_test": "Wasserstein distance (normed)",
      "drift_score": 0.371,
      "threshold": 0.1
    }
  ]
}
```

### **Phase 4: Evaluation Gate & Promotion** (`src/retraining/evaluation_gate.py`)
**Multi-criteria gate ensuring only safe models are promoted:**

| Gate # | Criterion | Threshold | Purpose |
|--------|-----------|-----------|---------|
| 1 | Sufficient samples | â‰¥200 | Statistical power |
| 2 | Label coverage | â‰¥30% | Evaluation validity |
| 3 | Promotion cooldown | 7 days | Deployment stability |
| 4 | F1 improvement | â‰¥2% | Business value |
| 5 | Calibration maintained | Brier â‰¤+0.01 | Probability quality |
| 6 | No segment regression | F1 drop â‰¤5% | Fairness & safety |

**Decision Logic**: ALL gates must pass â†’ Model promoted to production

## ðŸ“Š Data Flow

### Monitoring Pipeline (5-minute intervals)
```
Production Model
    â†“
Generate Predictions â†’ Log to CSV
    â†“
Collect Last 24h Predictions (â‰ˆ2000)
    â†“
Compute Proxy Metrics (coverage, patterns)
    â†“
Run Drift Detection (Evidently)
    â†“
Evaluate Against Reference Baseline
    â†“
[DRIFT DETECTED] â†’ Signal retraining workflow
    â†“
Save HTML Report + JSON Summary
```

### Retraining Pipeline (triggered by drift)
```
[Drift Signal] â†’ Trigger Retraining DAG (Airflow)
    â†“
Shadow Trainer: Train new model in parallel
    â†“
Wait for Labels (24-48h window)
    â†“
Evaluation Gate: Run 6-gate validation
    â†“
[ALL GATES PASS] â†’ Promote to Production
[GATE FAILS] â†’ Log rejection + wait
    â†“
Update Model Registry + Serve New Version
```

## ðŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.10
- Git

### Setup & Run
```bash
# Clone repository
git clone https://github.com/Nimish1106/self-healing-mlops.git
cd self-healing-mlops

# Start all services (API, monitoring, MLflow, Airflow)
docker-compose up -d

# Bootstrap reference data (frozen baseline)
docker exec reference-bootstrap python scripts/bootstrap_reference.py

# Generate synthetic predictions (for testing)
docker exec credit-risk-api python scripts/generate_fake_predictions.py

# Monitor logs
docker logs monitoring-scheduler -f        # Drift detection
docker logs model-trainer -f               # Retraining
docker logs credit-risk-api -f            # API predictions
```

### Access Services
- **API Predictions**: `http://localhost:8000/predict`
- **MLflow Tracking**: `http://localhost:5000`
- **Airflow DAGs**: `http://localhost:8080`
- **Drift Reports**: `/app/monitoring/reports/drift_reports/`

## ðŸ“ Directory Structure

```
self-healing-mlops/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_mlflow.py                  # FastAPI service
â”‚   â”œâ”€â”€ train_model_mlflow.py           # Training entrypoint
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ drift_detection.py          # Phase 3: Evidently drift
â”‚   â”‚   â”œâ”€â”€ drift_signals.py            # Drift â†’ retraining decision
â”‚   â”‚   â”œâ”€â”€ model_evaluator.py          # Shadow model evaluation
â”‚   â”‚   â””â”€â”€ proxy_metrics.py            # Coverage, patterns
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â””â”€â”€ monitoring_job.py           # 5-min scheduler
â”‚   â”œâ”€â”€ retraining/
â”‚   â”‚   â”œâ”€â”€ evaluation_gate.py          # Phase 4: 6-gate validation
â”‚   â”‚   â”œâ”€â”€ shadow_trainer.py           # Parallel training
â”‚   â”‚   â””â”€â”€ model_promoter.py           # Registry updates
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ prediction_logger.py        # CSV logging
â”‚   â”‚   â””â”€â”€ label_store.py              # Label collection
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ temporal_utils.py           # Time windows
â”‚       â””â”€â”€ dataset_fingerprint.py      # Data integrity
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ retraining_pipeline.py      # Orchestration DAG
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bootstrap_reference.py          # Freeze baseline
â”‚   â”œâ”€â”€ generate_fake_predictions.py    # Testing data
â”‚   â”œâ”€â”€ simulate_traffic.py             # Load simulation
â”‚   â””â”€â”€ run_retraining_workflow.py      # Manual trigger
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ reference/                      # Frozen baseline
â”‚   â”œâ”€â”€ predictions/                    # Production logs
â”‚   â”œâ”€â”€ labels/                         # True outcomes
â”‚   â”œâ”€â”€ metrics/                        # Evaluation results
â”‚   â””â”€â”€ reports/                        # Drift reports (HTML + JSON)
â”‚
â”œâ”€â”€ docker-compose.yml                 # Multi-service orchestration
â”œâ”€â”€ Dockerfile                         # Main app image
â””â”€â”€ Dockerfile.airflow                 # Airflow-specific image
```

## ðŸ” Key Design Decisions

### 1. **Drift Detection: Distribution, Not Performance**
- Evidently detects **IF** distributions changed
- Does NOT evaluate **IF** model performance degraded
- Drift â‰  model failure (must wait for labels)
- Prevents reactive retraining on transient shifts

### 2. **Evaluation Gate: All-or-Nothing**
- Every gate acts as a circuit breaker
- Single failure â†’ entire promotion blocked
- Rejection is **successful** system behavior
- No partial promotions or A/B testing

### 3. **Promotion Cooldown: Stability Over Optimization**
- 7-day minimum between promotions
- Prevents retraining storms
- Allows time to detect real-world issues
- Authority: EvaluationGate only (ModelPromoter trusted)

### 4. **Label Coverage: Practical Evaluation**
- Only ~30% of predictions get labels in 24h
- Minimum 30% coverage required for gate passage
- Balances waiting time vs. decision confidence
- Fail-closed if coverage_stats missing

### 5. **Segment Fairness: Explicit Checking**
- Feature â†’ segment â†’ performance tracked
- Detects if new model hurts minority groups
- Â±5% F1 drop tolerance per segment
- Non-blocking for missing segments (insufficient data)

## ðŸ“ˆ Monitoring & Observability

### Drift Summary Files
Location: `/app/monitoring/reports/drift_reports/`
- `drift_summary_YYYYMMDD_HHMMSS.json` - Machine-readable summary
- `drift_report_YYYYMMDD_HHMMSS.html` - Visual Evidently report

### Decision Records
Location: `/app/monitoring/retraining/decisions/`
- `decision_*.json` - Gate pass/fail records
- Contains: timestamp, all gate results, final decision, reason

### Monitoring Results
Location: `/app/monitoring/metrics/monitoring_results/`
- `monitoring_YYYYMMDD_HHMMSS.json` - Aggregated metrics
- Proxy coverage, feature statistics, data freshness

## ðŸ§ª Testing & Validation

### Generate Test Predictions
```bash
docker exec credit-risk-api python scripts/generate_fake_predictions.py
```

### Simulate Traffic
```bash
docker exec credit-risk-api python scripts/simulate_traffic.py --duration=3600
```

### Trigger Retraining Manually
```bash
docker exec model-trainer python scripts/run_retraining_workflow.py
```

## ðŸ“Š Implementation Metrics

| Component | Technology | Version |
|-----------|-----------|---------|
| Drift Detection | Evidently | 0.4.15 |
| ML Framework | scikit-learn | 1.3.2 |
| Feature Encoding | Categorical Encoding | 2.6.1 |
| Web Framework | FastAPI | 0.109.2 |
| Orchestration | Airflow | 2.7.3 |
| Model Registry | MLflow | 2.14.1 |
| Containerization | Docker | 27.x |
| Database | PostgreSQL | 15 |

## ðŸ›¡ï¸ Safety & Correctness

- âœ… **Phase 3 Complete**: Full drift detection with feature-level details
- âœ… **Phase 4 Complete**: 6-gate evaluation with cooldown enforcement
- âœ… **Reference Immutability**: Frozen baseline never modified
- âœ… **Fail-Closed Gating**: Missing data â†’ rejection, not bypass
- âœ… **Audit Trail**: All decisions logged with timestamps & rationale
- âœ… **Docker Isolation**: Clean layer builds, no cached code issues

## ðŸš¦ Deployment Readiness

- [x] Code quality: Comprehensive logging & error handling
- [x] Testing: Docker builds without cache, services verified working
- [x] Documentation: README, inline comments, decision rationale
- [x] Configuration: All paths, thresholds, intervals configurable
- [x] Monitoring: Full observability with JSON logs & HTML reports
- [x] Git history: Clean commits with feature branches

## ðŸ“ Recent Fixes (Phase 4)

### Critical Issue: Drift Summary Structure
**Problem**: JSON files missing feature-level details
- Old code: Only `drift_share`, `num_drifted_features`
- Expected: Complete feature array with drift scores, p-values

**Solution**: 
- Added `ColumnDriftMetric` for each feature
- Proper extraction from Evidently report structure
- Complete JSON with 10 required fields

**Validation**:
```bash
$ docker logs monitoring-scheduler 2>&1 | grep "Features array"
Drift summary | dataset_drift=True | drift_share=50.00% | drifted=6/10 evaluated (0 excluded)
Features array length: 10
âœ… Drift summary structure is complete
```

## ðŸ”— Related Documentation

- **Phase 1**: Baseline model training & development
- **Phase 2**: Production API & prediction logging  
- **Phase 3**: Drift detection using Evidently
- **Phase 4**: Evaluation gates & model promotion (this phase)

## ðŸ“ž Support

For issues or questions:
1. Check Docker logs: `docker-compose logs [service-name]`
2. Review drift reports: `/monitoring/reports/drift_reports/`
3. Check decision records: `/monitoring/retraining/decisions/`
4. Verify monitoring metrics: `/monitoring/metrics/monitoring_results/`

---

**Status**: âœ… Phase 4 Complete & Ready for Production  
**Last Updated**: 2026-01-04  
**Branch**: `phase-4-mlops`

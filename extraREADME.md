
## ğŸ’¡ Key Concepts

### 1ï¸âƒ£ Frozen Reference Data

<table>
<tr>
<td width="20%"><b>What</b></td>
<td>Training test set snapshot (last 20% of data)</td>
</tr>
<tr>
<td><b>Why</b></td>
<td>Drift is relative to this baseline</td>
</tr>
<tr>
<td><b>Status</b></td>
<td><code>IMMUTABLE</code> - Never modified after creation</td>
</tr>
<tr>
<td><b>Integrity</b></td>
<td>SHA256 hash verified on every monitoring run</td>
</tr>
</table>

```bash
# Re-bootstrap if needed (âš ï¸ destroys old reference)
docker-compose run --rm bootstrap
```

---

### 2ï¸âƒ£ Minimum Sample Requirement

<div align="center">

![Min Samples](https://img.shields.io/badge/Minimum_Samples-200-critical?style=for-the-badge)

</div>

**Why 200?**
- Statistical tests (KS, Chi-squared) need adequate power
- Central Limit Theorem validity
- Prevents false positives/negatives

**Below threshold:**
```json
{
  "status": "insufficient_data",
  "num_predictions": 87,
  "min_required": 200,
  "message": "Waiting for more predictions"
}
```

---

### 3ï¸âƒ£ Proxy Metrics (No Labels Needed)

<table>
<tr>
<th>Metric</th>
<th>What It Measures</th>
<th>Why It Matters</th>
</tr>
<tr>
<td>ğŸ“Š Prediction Distribution</td>
<td>Positive rate, probability stats</td>
<td>Detects output behavior changes</td>
</tr>
<tr>
<td>ğŸ² Probability Entropy</td>
<td>Model decisiveness</td>
<td>High entropy = uncertain predictions</td>
</tr>
<tr>
<td>â±ï¸ Time-Windowed Trends</td>
<td>1h, 6h, 24h windows</td>
<td>Detect sudden vs gradual changes</td>
</tr>
<tr>
<td>ğŸ“ˆ Rate of Change</td>
<td>How fast metrics shift</td>
<td>Urgency indicator</td>
</tr>
</table>

**âŒ Not Included:** Accuracy, Precision, Recall (require labels)

---

### 4ï¸âƒ£ Drift Detection

<div align="center">

### ğŸ¯ What Drift Tells Us

| âœ… What Evidently Detects | âŒ What It Does NOT Tell |
|---------------------------|--------------------------|
| Feature distributions changed | Model is wrong |
| Statistical test results | Need to retrain |
| Which features drifted | Change is "bad" |

</div>

<br/>

> **âš ï¸ Critical Understanding**  
> **Drift â‰  Model Failure**

<br/>

**Possible Interpretations:**

| Scenario | Interpretation | Action |
|----------|----------------|--------|
| ğŸŒ World changed | Model might be outdated | Consider retraining (Phase 4) |
| ğŸ”§ Data quality issue | Pipeline problem | Fix data, not model |
| ğŸ“… Seasonality | Expected variation | No action needed |
| ğŸ‘¥ New segment | Working as designed | Model is fine |

---

### 5ï¸âƒ£ Observation vs Action

<table>
<tr>
<td width="50%" bgcolor="#E3F2FD">

### ğŸ“Š Phase 3 (Current)
**Observes & Reports**

- âœ… Logs predictions
- âœ… Computes statistics
- âœ… Detects drift
- âœ… Generates reports
- âœ… Creates signals

</td>
<td width="50%" bgcolor="#FFF3E0">

### ğŸš€ Phase 4 (Next)
**Decides & Acts**

- ğŸ”„ Evaluates signals
- ğŸ”„ Trains shadow models
- ğŸ”„ Runs evaluation gates
- ğŸ”„ Promotes models
- ğŸ”„ Automated retraining

</td>
</tr>
</table>

---

## âš™ï¸ Configuration

### Environment Variables

Edit `docker-compose.yml`:

```yaml
monitoring:
  environment:
    - MONITORING_INTERVAL=300    # Seconds between runs
    - MONITORING_LOOKBACK=24     # Hours of data to analyze
```

### ğŸ“Š Recommended Settings

<table>
<tr>
<th>Prediction Volume</th>
<th>Interval</th>
<th>Lookback</th>
<th>Rationale</th>
</tr>
<tr>
<td>ğŸŒ Low (<100/day)</td>
<td>3600s (1h)</td>
<td>168h (7d)</td>
<td>Need longer window for stats</td>
</tr>
<tr>
<td>ğŸš¶ Medium (100-1K/day)</td>
<td>600s (10m)</td>
<td>24h</td>
<td>Balance responsiveness & validity</td>
</tr>
<tr>
<td>ğŸš€ High (>1K/day)</td>
<td>300s (5m)</td>
<td>6h</td>
<td>Fast drift detection possible</td>
</tr>
</table>

---

## ğŸŒ API Endpoints

<table>
<tr>
<th>Endpoint</th>
<th>Method</th>
<th>Purpose</th>
<th>Status</th>
</tr>
<tr>
<td><code>/</code></td>
<td>GET</td>
<td>Basic health check</td>
<td>âœ… Public</td>
</tr>
<tr>
<td><code>/health</code></td>
<td>GET</td>
<td>Detailed status + prediction count</td>
<td>âœ… Public</td>
</tr>
<tr>
<td><code>/model/info</code></td>
<td>GET</td>
<td>Current model metadata</td>
<td>âœ… Public</td>
</tr>
<tr>
<td><code>/predict</code></td>
<td>POST</td>
<td>Make prediction + log</td>
<td>â­ Primary</td>
</tr>
<tr>
<td><code>/monitoring/stats</code></td>
<td>GET</td>
<td>Quick stats (last 100)</td>
<td>ğŸ“Š Monitoring</td>
</tr>
</table>

### Example Request

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "RevolvingUtilizationOfUnsecuredLines": 0.766127,
    "age": 45,
    "NumberOfTime30_59DaysPastDueNotWorse": 2,
    "DebtRatio": 0.802982,
    "MonthlyIncome": 9120.0,
    "NumberOfOpenCreditLinesAndLoans": 13,
    "NumberOfTimes90DaysLate": 0,
    "NumberRealEstateLoansOrLines": 6,
    "NumberOfTime60_89DaysPastDueNotWorse": 0,
    "NumberOfDependents": 2
  }'
```

### Example Response

```json
{
  "prediction": 0,
  "probability": 0.087,
  "model_version": "1",
  "prediction_id": "pred_20240115_143022_123456",
  "timestamp": "2024-01-15T14:30:22.123456"
}
```

---

## ğŸ“Š Monitoring Outputs

### JSON Results

```json
{
  "timestamp": "2024-01-15T14:30:00",
  "analysis_window": {
    "lookback_hours": 24,
    "start_time": "2024-01-14T14:30:00",
    "end_time": "2024-01-15T14:30:00",
    "num_predictions": 1247
  },
  "proxy_metrics": {
    "overall_stats": {
      "positive_rate": 0.073,
      "probability_mean": 0.089,
      "probability_std": 0.142
    },
    "entropy": 1.23,
    "time_windowed": {
      "window_1H": {"positive_rate": 0.075},
      "window_6H": {"positive_rate": 0.072},
      "window_24H": {"positive_rate": 0.073}
    }
  },
  "drift_detection": {
    "drift_share": 0.30,
    "num_drifted_features": 3,
    "num_features_total": 10,
    "feature_drift_details": [
      {
        "feature": "MonthlyIncome",
        "drift_detected": true,
        "drift_score": 0.045,
        "stat_test": "ks"
      }
    ]
  }
}
```

### ğŸ“„ HTML Reports

**Location:** `monitoring/reports/drift_reports/drift_report_*.html`

**Contents:**
- ğŸ“Š Per-feature drift analysis
- ğŸ“ˆ Distribution comparisons (histograms)
- ğŸ“‰ Statistical test results
- ğŸ¨ Interactive visualizations

```bash
# Open latest report
open $(ls -t monitoring/reports/drift_reports/*.html | head -1)
```

---

## âœ… Verification

### Automated Check Script

```bash
#!/bin/bash
echo "ğŸ” Phase 3 Verification"
echo "======================="

# 1. Services
echo -n "1. Services running... "
docker-compose ps | grep -q "Up" && echo "âœ…" || echo "âŒ"

# 2. Reference
echo -n "2. Reference exists... "
test -f monitoring/reference/reference_data.csv && echo "âœ…" || echo "âŒ"

# 3. Predictions
echo -n "3. Predictions logged... "
test $(wc -l < monitoring/predictions/predictions.csv) -gt 200 && echo "âœ…" || echo "âŒ"

# 4. Results
echo -n "4. Monitoring results... "
test -n "$(ls monitoring/metrics/monitoring_results/ 2>/dev/null)" && echo "âœ…" || echo "âŒ"

# 5. API health
echo -n "5. API healthy... "
curl -sf http://localhost:8000/health > /dev/null && echo "âœ…" || echo "âŒ"

echo "======================="
```

### Manual Checklist

- [ ] ğŸ³ All Docker services show "Up" status
- [ ] ğŸ“¦ Reference data exists and has metadata
- [ ] ğŸ“ Predictions CSV has 200+ lines
- [ ] ğŸ“Š Monitoring results directory not empty
- [ ] ğŸŒ API returns 200 on `/health`
- [ ] ğŸ” No ERROR logs in monitoring service
- [ ] ğŸ“ˆ Drift reports generated in HTML
- [ ] â„¹ï¸ Logs show INFO-level observations (not warnings)

---

## ğŸ”§ Troubleshooting

### Issue: "Insufficient samples" error

<details>
<summary><b>Show solution</b></summary>

**Symptom:**
```json
{"status": "insufficient_data", "num_predictions": 87}
```

**Diagnosis:**
```bash
# Check current count
curl http://localhost:8000/monitoring/stats | jq '.total_predictions'
```

**Fix:**
```bash
# Generate more predictions (see Quick Start Step 5)
# Need at least 200 for statistical validity
```

</details>

---

### Issue: "Reference data not found"

<details>
<summary><b>Show solution</b></summary>

**Symptom:**
```
FileNotFoundError: Reference data not found
```

**Fix:**
```bash
# Bootstrap reference data
docker-compose run --rm bootstrap

# Verify creation
ls -la monitoring/reference/
```

</details>

---

### Issue: "Reference integrity check failed"

<details>
<summary><b>Show solution</b></summary>

**Symptom:**
```
ValueError: Reference data integrity check FAILED
```

**Cause:** reference_data.csv was accidentally modified

**Fix:**
```bash
# Re-bootstrap (âš ï¸ will overwrite existing reference)
docker-compose run --rm bootstrap

# Restart monitoring
docker-compose restart monitoring
```

</details>

---

### Issue: Monitoring service not running

<details>
<summary><b>Show solution</b></summary>

**Diagnosis:**
```bash
# Check status
docker-compose ps monitoring

# View logs
docker-compose logs --tail=100 monitoring
```

**Common Causes:**
1. Reference data not bootstrapped
2. MLflow not accessible
3. Code syntax error

**Fix:**
```bash
# Rebuild and restart
docker-compose up -d --build monitoring

# Follow logs
docker-compose logs -f monitoring
```

</details>

---

### Issue: No drift detected

<details>
<summary><b>Show solution</b></summary>

**Expected Behavior:** âœ… This is CORRECT

With static test data, you won't see drift. Drift requires:
- Actual distribution changes
- Different data sources
- Time-based variation

**To test drift detection:**
1. Modify input data distributions
2. Use different dataset segment
3. Wait for real production data variation

</details>

---

## ğŸ“š Important Concepts

### Data Drift vs Concept Drift

<table>
<tr>
<th width="50%">ğŸ“Š Data Drift (Covariate Shift)</th>
<th width="50%">ğŸ”„ Concept Drift</th>
</tr>
<tr>
<td>

**What changes:** Input features (X)

**Example:**
- Average income increases 20%
- Age distribution shifts older
- New geographic regions

**Phase 3 detects:** âœ… YES

</td>
<td>

**What changes:** Xâ†’Y relationship

**Example:**
- Credit score becomes less predictive
- Economic crisis changes default patterns
- Regulatory changes

**Phase 3 detects:** âŒ NO (needs labels)

</td>
</tr>
</table>

---

### Why No Accuracy Metrics?

<div align="center">

### â° Labels Arrive Later

</div>

**Example Timeline:**
```
Day 1:  Make prediction (will they default?)
        â†“
Day 1:  Log prediction for monitoring âœ…
        â†“
Month 6: Ground truth arrives (did they default?)
        â†“
Month 6: Compute accuracy (Phase 4+) â³
```

**Solution:** Phase 3 uses **proxy metrics** that work without labels.

**Phase 4** will add **delayed evaluation** when labels arrive.

---

## ğŸ“¦ Dependencies

```txt
# Phase 3 Additions
evidently==0.4.11    # Drift detection framework
scipy==1.11.4        # Statistical tests
plotly==5.18.0       # Evidently visualizations

# See requirements.txt for full dependency list
```

---

## ğŸ® Quick Commands

```bash
# ğŸš€ Start everything
docker-compose up -d

# ğŸ”„ Restart service
docker-compose restart monitoring

# ğŸ”¨ Rebuild and restart
docker-compose up -d --build monitoring

# ğŸ“‹ View logs
docker-compose logs -f monitoring

# âš¡ Force monitoring run
docker-compose exec monitoring python src/monitoring/monitoring_job.py

# ğŸ” Check service status
docker-compose ps

# ğŸ§¹ Clean slate (âš ï¸ deletes data)
docker-compose down -v

# ğŸ“Š View latest results
ls -t monitoring/metrics/monitoring_results/*.json | head -1 | xargs cat | jq
```

---
